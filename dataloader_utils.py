# !/usr/bin/env python3
# -*- coding: utf-8 -*-
import json
import random
from multiprocessing import Pool
import functools
import numpy as np
from collections import defaultdict
from itertools import chain

from utils import Label2IdxSub, Label2IdxObj


class InputExample(object):
    """a single set of samples of data
    """

    def __init__(self, text, en_pair_list, re_list, rel2ens):
        self.text = text
        self.en_pair_list = en_pair_list
        self.re_list = re_list
        self.rel2ens = rel2ens


class InputFeatures(object):
    """
    Desc:
        a single set of features of data
    """

    def __init__(self,
                 input_tokens,
                 input_ids,
                 attention_mask,
                 seq_tag=None,
                 corres_tag=None,
                 relation=None,
                 triples=None,
                 rel_tag=None
                 ):
        self.input_tokens = input_tokens
        self.input_ids = input_ids
        self.attention_mask = attention_mask
        self.seq_tag = seq_tag
        self.corres_tag = corres_tag
        self.relation = relation
        self.triples = triples
        self.rel_tag = rel_tag


def read_examples(data_dir, data_sign, rel2idx):
    """load data to InputExamples  è¿™é‡Œå°±æ˜¯è·å–æ•°æ®
    """
    examples = []

    # read src data
    with open(data_dir / f'{data_sign}_triples.json', "r", encoding='utf-8') as f:
        data = json.load(f)
        for sample in data:
            text = sample['text']
            rel2ens = defaultdict(list)
            en_pair_list = []
            re_list = []

            for triple in sample['triple_list']:
                en_pair_list.append([triple[0], triple[-1]])    # å®ä½“å¯¹
                re_list.append(rel2idx[triple[1]])  # å…³ç³»åˆ—è¡¨
                rel2ens[rel2idx[triple[1]]].append(
                    (triple[0], triple[-1]))  # å…³ç³»å¯¹åº”çš„å®ä½“å¯¹
            # ğŸ‘‡ğŸ»è¿™ä¸ªåªæ˜¯æŠŠä¸€ä¸ªtextçš„æ”¾å…¥äº†ä¸€ä¸ªæ ·æœ¬ä¸­
            example = InputExample(
                text=text, en_pair_list=en_pair_list, re_list=re_list, rel2ens=rel2ens)
            examples.append(example)
    print("InputExamples:", len(examples))
    return examples


def find_head_idx(source, target):
    """åœ¨listä¸­æœç´¢ç¬¬ä¸€ä¸ªå‡ºç°çš„å­é›†,è¿™ä¸ªä»£ç å†™çš„è¿˜ä¸é”™

    :param source: å­é›†
    :type source: list
    :param target: éœ€è¦æŸ¥æ‰¾çš„list
    :type target: list
    :return: ç¬¬ä¸€ä¸ªå‡ºç°å­é›†çš„ä½ç½®
    :rtype: int, æ²¡æœ‰æœç´¢åˆ°å°±è¿”å›-1
    """
    target_len = len(target)
    for i in range(len(source)):
        if source[i: i + target_len] == target:
            return i
    return -1


def _get_so_head(en_pair, tokenizer, text_tokens):
    """è·å–å®ä½“å¯¹ä¸­subjectå’Œobjectåœ¨text_tokensè¿™ä¸ªlistä¸­å‡ºç°çš„ç¬¬ä¸€ä¸ªä½ç½®

    :param en_pair: å®ä½“å¯¹,lenä¸º2,ä¸€ä¸ªsubject,ä¸€ä¸ªobject
    :type en_pair: list
    :param tokenizer: Transformer tokenizer
    :type tokenizer: Transformer tokenizer
    :param text_tokens: è¿™ä¸ªæ˜¯å°†ä¹‹å‰çš„textè½¬æ¢æˆäº†ä¸€ä¸ªlist
    :type text_tokens: list
    :return: åˆ†åˆ«æ˜¯ subjectåœ¨text_tokensä¸­ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®,objectåœ¨text_tokensä¸­ç¬¬ä¸€æ¬¡å‡ºç°çš„ä½ç½®,ä»¥åŠsubçš„listå’Œobjçš„list
    :rtype: set
    """
    sub = tokenizer.tokenize(en_pair[0])
    obj = tokenizer.tokenize(en_pair[1])
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªsubjectçš„ä½ç½®,è¿™ä¸ªä½ç½®å¯¹åº”çš„æ˜¯text_tokens è¿™ä¸ªlistçš„index
    sub_head = find_head_idx(source=text_tokens, target=sub)
    if sub == obj:  # è¿™é‡Œè€ƒè™‘äº†ä¸¤ä¸ªå®ä½“ä¸€æ ·
        obj_head = find_head_idx(
            source=text_tokens[sub_head + len(sub):], target=obj)
        if obj_head != -1:
            obj_head += sub_head + len(sub)
        else:
            obj_head = sub_head
    else:
        obj_head = find_head_idx(source=text_tokens, target=obj)
    return sub_head, obj_head, sub, obj


def convert(example, max_text_len, tokenizer, rel2idx, data_sign, ex_params):
    """è¿™é‡Œå°±æ˜¯ä¸€ä¸ªexample,ä¸€ä¸ªexampleçš„å¤„ç†äº†,è¿™é‡Œæ‰æ˜¯æœ€é‡è¦çš„åœ°æ–¹

    :param example: ä¸€æ¡è®­ç»ƒæ•°æ®,ä¹Ÿå°±æ˜¯ä¸€å¥è¯,é‡Œé¢æœ‰å®ä½“åˆ—è¡¨ å…³ç³»åˆ—è¡¨ ä»¥åŠå…³ç³»å¯¹åº”çš„å®ä½“å¯¹
    :type example: InputeExample
    :param max_text_len: æ–‡å­—çš„æœ€å¤§é•¿åº¦,å¤ªé•¿äº†å°±ä¼šç›´æ¥æˆªå–
    :type max_text_len: int
    :param tokenizer: Transformerçš„tokenizer
    :type tokenizer: Transformerçš„tokenizer
    :param rel2idx: ç¦»æ•£å…³ç³»å¯¹åº”çš„æ•°å€¼
    :type rel2idx: dict
    :param data_sign: æ‰€å±çš„dataç±»åˆ«(test,train,val)
    :type data_sign: str
    :param ex_params: TODO
    :type ex_params: dict
    :return: [description] TODO
    :rtype: list or InputFeatures,InputFeatures(input_tokens,   # å°±æ˜¯è¿™å¥è¯çš„token,è¿™é‡Œçš„tokenå°±æ˜¯è‹±æ–‡åˆ†è¯åçš„è‹±æ–‡,è¿™é‡Œæ³¨æ„å’¯,è¿™é‡Œä¸æ˜¯å‘é‡!!!
                                                input_ids,      # è¿™é‡Œéƒ½æ˜¯0-1çš„å€¼,è¿™é‡Œä¸»è¦æ˜¯ä½ç½®çš„mask
                                                attention_mask, # è¿™é‡Œæ˜¯attention mask
                                                seq_tag=None,   # è¿™ä¸ªé‡Œé¢è£…äº†ä¸¤ä¸ªlist,ä¸€ä¸ªæ˜¯sub,ä¸€ä¸ªæ˜¯objçš„,é‡Œé¢çš„å–å€¼æœ‰ä¸‰ç§012 0å°±æ˜¯ä¸æ˜¯å®ä½“ 1å°±æ˜¯å®ä½“çš„å¼€å§‹ 2å°±æ˜¯å®ä½“çš„å»¶ç»­
                                                corres_tag=None,
                                                relation=None,
                                                triples=None,
                                                rel_tag=None)
    """
    text_tokens = tokenizer.tokenize(
        example.text)  # è¿™ä¸ªæ–¹æ³•ä¸å¤ªçŸ¥é“æ˜¯å¹²å˜›çš„,åªæ˜¯çŸ¥é“ä»–è¿™é‡Œå¯èƒ½æ˜¯åˆ†è¯å§...  è¿™é‡Œå°±æ˜¯æŠŠstr=>list
    # cut off
    if len(text_tokens) > max_text_len:
        text_tokens = text_tokens[:max_text_len]

    # token to id
    input_ids = tokenizer.convert_tokens_to_ids(text_tokens)
    attention_mask = [1] * len(input_ids)
    # zero-padding up to the sequence length
    # è¿™é‡Œå…¶å®å°±æ˜¯åšäº†ä¸€ä¸ªä½ç½®çš„mask, è¿™é‡Œéœ€è¦æ³¨æ„è¿™ä¸ªmaskå’Œåé¢çš„masked attentionçš„maskç”¨æ³•å¯ä¸æ˜¯åŒä¸€ç§ææ³•,
    # ä¸‹é¢å…¶å®å°±æ˜¯å¦‚æœä»€ä¹ˆä½ç½®ä¸å¤Ÿå°±è¡¥æˆ0
    if len(input_ids) < max_text_len:
        pad_len = max_text_len - len(input_ids)
        # token_pad_id=0
        input_ids += [0] * pad_len
        attention_mask += [0] * pad_len

    # train data
    if data_sign == 'train':
        # construct tags of correspondence and relation  è¿™é‡Œå¾ˆå…³é”®å‘€,è¿™ä¸ªå°±æ˜¯é‚£ä¸ªcorrespondence and relationçš„çŸ©é˜µ
        # ä¹‹åä¼šç”¨å®ƒå»åšä¸€ä¸ªå‰”é™¤çš„å·¥ä½œ!!!
        corres_tag = np.zeros((max_text_len, max_text_len))
        rel_tag = len(rel2idx) * [0]    # è¿™é‡Œå°±æ˜¯è®ºæ–‡é‡Œé¢é¢„æµ‹å…³ç³»çš„list
        # è¿™ä¸ªåœ°æ–¹çš„å·¥ä½œåªæ˜¯ä¸ºäº†å°†é‚£ä¸ªconstruct tags of correspondence and relationçš„å€¼ç»™èµ‹å€¼å¥½
        for en_pair, rel in zip(example.en_pair_list, example.re_list):
            # get sub and obj head
            sub_head, obj_head, _, _ = _get_so_head(
                en_pair, tokenizer, text_tokens)
            # construct relation tag
            rel_tag[rel] = 1
            if sub_head != -1 and obj_head != -1:
                # è¿™é‡Œå®ƒå°±æŠŠæœ‰å…³ç³»çš„éƒ½æ”¾è¿›å»äº†,å¹¶èµ‹å€¼ä¸º1 TODO è¿™é‡Œæˆ‘æœ‰ä¸€ä¸ªé—®é¢˜,é‚£è¿™é‡Œå¤šé‡å…³ç³»,è¿™é‡Œå§‹ç»ˆéƒ½æ˜¯1å‘€...
                corres_tag[sub_head][obj_head] = 1

        sub_feats = []
        # positive samples
        for rel, en_ll in example.rel2ens.items():
            # init
            tags_sub = max_text_len * [Label2IdxSub['O']]
            tags_obj = max_text_len * [Label2IdxSub['O']]
            for en in en_ll:
                # get sub and obj head
                sub_head, obj_head, sub, obj = _get_so_head(
                    en, tokenizer, text_tokens)
                if sub_head != -1 and obj_head != -1:
                    if sub_head + len(sub) <= max_text_len:
                        tags_sub[sub_head] = Label2IdxSub['B-H']
                        tags_sub[sub_head + 1:sub_head +
                                 len(sub)] = (len(sub) - 1) * [Label2IdxSub['I-H']]
                    if obj_head + len(obj) <= max_text_len:
                        tags_obj[obj_head] = Label2IdxObj['B-T']
                        tags_obj[obj_head + 1:obj_head +
                                 len(obj)] = (len(obj) - 1) * [Label2IdxObj['I-T']]
            # ä¸Šé¢æŠŠsubå’Œobjåˆ†æˆäº†ä¸¤ä¸ªéƒ¨åˆ†,ä¸çŸ¥é“è¿™é‡Œçš„subå’Œobjçš„è¯†åˆ«æ˜¯ä¸æ˜¯åšäº†ä¸¤ä¸ªåˆ†ç±»,è€Œä¸”è¿™ä¸ªè¶…çº§åƒCasé‚£ç¯‡æ–‡ç« çš„æ€æƒ³
            seq_tag = [tags_sub, tags_obj]

            # sanity check TODO è¿™ä¸ªå¯ä»¥å­¦ä¹ ä¸€ä¸‹
            assert len(input_ids) == len(tags_sub) == len(tags_obj) == len(
                attention_mask) == max_text_len, f'length is not equal!!'
            # è¿™é‡Œéœ€è¦æ³¨æ„,è¿™é‡Œçš„sub_featsæ˜¯ä»¥ä¸€ä¸ªå…³ç³»ä¸ºä¸€ä¸ªç‰¹å¾çš„,ç„¶åè¿™ä¸ªå…³ç³»ä¸­å¯èƒ½å¸¦ç€å¾ˆå¤šå®ä½“!
            sub_feats.append(InputFeatures(
                input_tokens=text_tokens,
                input_ids=input_ids,
                attention_mask=attention_mask,
                corres_tag=corres_tag,
                seq_tag=seq_tag,
                relation=rel,
                rel_tag=rel_tag
            ))
        # relation judgement ablation å…³ç³»åˆ¤æ–­æ¶ˆè  è¿™ä¸ªæ¶ˆèçš„æ¦‚å¿µæ˜¯ä»€ä¹ˆ??? TODO
        if not ex_params['ensure_rel']:
            # negative samples
            neg_rels = set(rel2idx.values()).difference(set(example.re_list))
            neg_rels = random.sample(neg_rels, k=ex_params['num_negs'])
            for neg_rel in neg_rels:
                # init
                seq_tag = max_text_len * [Label2IdxSub['O']]
                # sanity check
                assert len(input_ids) == len(seq_tag) == len(
                    attention_mask) == max_text_len, f'length is not equal!!'
                seq_tag = [seq_tag, seq_tag]
                sub_feats.append(InputFeatures(
                    input_tokens=text_tokens,
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    corres_tag=corres_tag,
                    seq_tag=seq_tag,
                    relation=neg_rel,
                    rel_tag=rel_tag
                ))
    # val and test data
    else:
        triples = []
        for rel, en in zip(example.re_list, example.en_pair_list):  # è¿™ä¸ªzipçš„æ–¹æ³•è¦å¸æ”¶ä¸‹æ¥,æŒºå¥½ç”¨çš„ TODO
            # get sub and obj head
            sub_head, obj_head, sub, obj = _get_so_head(
                en, tokenizer, text_tokens)
            if sub_head != -1 and obj_head != -1:
                h_chunk = ('H', sub_head, sub_head + len(sub))
                t_chunk = ('T', obj_head, obj_head + len(obj))
                triples.append((h_chunk, t_chunk, rel))  # è¿™å¯èƒ½å°±æ˜¯ä¸€ä¸ªä¸‰å…ƒç»„å§
        sub_feats = [
            InputFeatures(
                input_tokens=text_tokens,
                input_ids=input_ids,
                attention_mask=attention_mask,
                triples=triples
            )
        ]

    # get sub-feats
    return sub_feats


def convert_examples_to_features(params, examples, tokenizer, rel2idx, data_sign, ex_params) -> list:
    """å°†exampleçš„æ•°æ®è½¬æ¢æˆä¸ºfeaturesæ”¾å…¥æ¨¡å‹,è¿™é‡Œç”¨äº†å¤šçº¿ç¨‹å»convertæ•°æ®

    :param params: è¿™ä¸ªéƒ½æ˜¯è‡ªå®šä¹‰å¥½çš„,éƒ½æ˜¯å†™åˆ°äº†./utils.pyé‡Œé¢
    :type params: dict
    :param examples: è¿™é‡Œå°†exampleè½¬æˆäº†å¯¹è±¡,InputExample(text=text, en_pair_list=en_pair_list, re_list=re_list, rel2ens=rel2ens)
    :type examples: InputExample
    :param tokenizer: ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç›´æ¥å‡ºæ¥çš„tokenizer
    :type tokenizer: Transformer Tokenizer
    :param rel2idx: å…³ç³»å¯¹åº”çš„å…³ç³»,ä¾‹å¦‚ "å›½å®¶ä¸»å¸­"=>1, å¯ä»¥ç†è§£æˆæ˜¯å°†ç¦»æ•£çš„å€¼è½¬æ¢æˆä¸ºæ•°å€¼
    :type rel2idx: dict
    :param data_sign: æ˜¯train,test,valå“ªä¸€ç§,ä¾‹å¦‚"train"
    :type data_sign: str
    :param ex_params: é»˜è®¤æ˜¯NULL
    :type ex_params: [type]
    :return: å…·ä½“ç‰¹å¾
    :rtype: list
    """
    max_text_len = params.max_seq_length
    # multi-process  è¿™é‡Œä½¿ç”¨å¤šè¿›ç¨‹çš„æ–¹å¼å€¼çš„å­¦ä¹  TODO
    with Pool(10) as p:
        convert_func = functools.partial(convert, max_text_len=max_text_len, tokenizer=tokenizer, rel2idx=rel2idx,
                                         data_sign=data_sign, ex_params=ex_params)
        features = p.map(func=convert_func, iterable=examples)

    return list(chain(*features))
